{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXzSzzZjGhEr"
      },
      "source": [
        "# Semantic Chunking for Document Processing\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a semantic chunking approach for processing and retrieving information from PDF documents, [first proposed by Greg Kamradt](https://youtu.be/8OJC21T2SL4?t=1933) and subsequently [implemented in LangChain](https://python.langchain.com/docs/how_to/semantic-chunker/). Unlike traditional methods that split text based on fixed character or word counts, semantic chunking aims to create more meaningful and context-aware text segments.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional text splitting methods often break documents at arbitrary points, potentially disrupting the flow of information and context. Semantic chunking addresses this issue by attempting to split text at more natural breakpoints, preserving semantic coherence within each chunk.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text extraction\n",
        "2. Semantic chunking using LangChain's SemanticChunker\n",
        "3. Vector store creation using FAISS and OpenAI embeddings\n",
        "4. Retriever setup for querying the processed documents\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The PDF is read and converted to a string using a custom `read_pdf_to_string` function.\n",
        "\n",
        "### Semantic Chunking\n",
        "\n",
        "1. Utilizes LangChain's `SemanticChunker` with OpenAI embeddings.\n",
        "2. Three breakpoint types are available:\n",
        "   - 'percentile': Splits at differences greater than the X percentile.\n",
        "   - 'standard_deviation': Splits at differences greater than X standard deviations.\n",
        "   - 'interquartile': Uses the interquartile distance to determine split points.\n",
        "3. In this implementation, the 'percentile' method is used with a threshold of 90.\n",
        "\n",
        "### Vector Store Creation\n",
        "\n",
        "1. OpenAI embeddings are used to create vector representations of the semantic chunks.\n",
        "2. A FAISS vector store is created from these embeddings for efficient similarity search.\n",
        "\n",
        "### Retriever Setup\n",
        "\n",
        "1. A retriever is configured to fetch the top 2 most relevant chunks for a given query.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. Context-Aware Splitting: Attempts to maintain semantic coherence within chunks.\n",
        "2. Flexible Configuration: Allows for different breakpoint types and thresholds.\n",
        "3. Integration with Advanced NLP Tools: Uses OpenAI embeddings for both chunking and retrieval.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved Coherence: Chunks are more likely to contain complete thoughts or ideas.\n",
        "2. Better Retrieval Relevance: By preserving context, retrieval accuracy may be enhanced.\n",
        "3. Adaptability: The chunking method can be adjusted based on the nature of the documents and retrieval needs.\n",
        "4. Potential for Better Understanding: LLMs or downstream tasks may perform better with more coherent text segments.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "1. Uses OpenAI's embeddings for both the semantic chunking process and the final vector representations.\n",
        "2. Employs FAISS for creating an efficient searchable index of the chunks.\n",
        "3. The retriever is set up to return the top 2 most relevant chunks, which can be adjusted as needed.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "The code includes a test query: \"What is the main cause of climate change?\". This demonstrates how the semantic chunking and retrieval system can be used to find relevant information from the processed document.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Semantic chunking represents an advanced approach to document processing for retrieval systems. By attempting to maintain semantic coherence within text segments, it has the potential to improve the quality of retrieved information and enhance the performance of downstream NLP tasks. This technique is particularly valuable for processing long, complex documents where maintaining context is crucial, such as scientific papers, legal documents, or comprehensive reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbM_FTp_GhEw"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/semantic_chunking_comparison.svg?raw=1\" alt=\"Self RAG\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xY-y3rGhEx"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BTQhSHIHGhE0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-experimental\n",
            "  Obtaining dependency information for langchain-experimental from https://files.pythonhosted.org/packages/24/fa/fb2c8b6418e1c9ef50c82b3b6e0184bce321582577240bb4b8ed3274a4aa/langchain_experimental-0.4.1-py3-none-any.whl.metadata\n",
            "  Downloading langchain_experimental-0.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting langchain-openai\n",
            "  Obtaining dependency information for langchain-openai from https://files.pythonhosted.org/packages/64/a1/50e7596aca775d8c3883eceeaf47489fac26c57c1abe243c00174f715a8a/langchain_openai-1.1.7-py3-none-any.whl.metadata\n",
            "  Using cached langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dotenv in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-experimental) (1.2.7)\n",
            "Collecting langchain-community<1.0.0,>=0.4.0 (from langchain-experimental)\n",
            "  Obtaining dependency information for langchain-community<1.0.0,>=0.4.0 from https://files.pythonhosted.org/packages/f0/a4/c4fde67f193401512337456cabc2148f2c43316e445f5decd9f8806e2992/langchain_community-0.4.1-py3-none-any.whl.metadata\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-openai) (2.15.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for langchain-classic<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/83/0f/eab87f017d7fe28e8c11fff614f4cdbfae32baadb77d0f79e9f922af1df2/langchain_classic-1.0.1-py3-none-any.whl.metadata\n",
            "  Using cached langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for SQLAlchemy<3.0.0,>=1.4.0 from https://files.pythonhosted.org/packages/bc/2a/2821a45742073fc0331dc132552b30de68ba9563230853437cac54b2b53e/sqlalchemy-2.0.46-cp312-cp312-win_amd64.whl.metadata\n",
            "  Downloading sqlalchemy-2.0.46-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.0.3)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/d9/b7/76175c7cb4eb73d91ad63c34e29fc4f77c9386bba4a65b53ba8e05ee3c39/aiohttp-3.13.3-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached aiohttp-3.13.3-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for dataclasses-json<0.7.0,>=0.6.7 from https://files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl.metadata\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for pydantic-settings<3.0.0,>=2.10.1 from https://files.pythonhosted.org/packages/c1/60/5d4751ba3f4a40a6891f24eec885f51afd78d208498268c734e256fb13c4/pydantic_settings-2.12.0-py3-none-any.whl.metadata\n",
            "  Using cached pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for httpx-sse<1.0.0,>=0.4.0 from https://files.pythonhosted.org/packages/d2/fd/6668e5aec43ab844de6fc74927e155a3b37bf40d7c3790e49fc0406b6578/httpx_sse-0.4.3-py3-none-any.whl.metadata\n",
            "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting numpy>=1.26.2 (from langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for numpy>=1.26.2 from https://files.pythonhosted.org/packages/51/cf/52a703dbeb0c65807540d29699fef5fda073434ff61846a564d5c296420f/numpy-2.4.1-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached numpy-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.12.5)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2026.1.15)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for aiohappyeyeballs>=2.5.0 from https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for aiosignal>=1.4.0 from https://files.pythonhosted.org/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl.metadata\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for attrs>=17.3.0 from https://files.pythonhosted.org/packages/3a/2a/7cc015f5b9f5db42b7d48157e23356022889fc354a2813c15934b7cb5c0e/attrs-25.4.0-py3-none-any.whl.metadata\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b8/af/38e51a553dd66eb064cdf193841f16f077585d4d28394c2fa6235cb41765/frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/46/e2/348cd32faad84eaf1d20cce80e2bb0ef8d312c55bca1f7fa9865e7770aaf/multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/54/09/d19cff2a5aaac632ec8fc03737b223597b1e347416934c1b3a7df079784c/propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/c8/9a/6ad1a9b37c2f72874f93e691b2e7ecb6137fb2b899983125db4204e47575/yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata\n",
            "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/be/2f/5108cb3ee4ba6501748c4908b908e55f42a5b66245b4cfe0c99326e1ef6e/marshmallow-3.26.2-py3-none-any.whl.metadata\n",
            "  Using cached marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.3)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for greenlet>=1 from https://files.pythonhosted.org/packages/34/2f/5e0e41f33c69655300a5e54aeb637cf8ff57f1786a3aba374eacc0228c1d/greenlet-3.3.1-cp312-cp312-win_amd64.whl.metadata\n",
            "  Downloading greenlet-3.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: colorama in c:\\my_data\\university-courses\\aie-intern\\trainnee\\llm&rag\\chunking-strategy\\venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental)\n",
            "  Obtaining dependency information for mypy-extensions>=0.3.0 from https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl.metadata\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_experimental-0.4.1-py3-none-any.whl (210 kB)\n",
            "   ---------------------------------------- 0.0/210.1 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 30.7/210.1 kB 660.6 kB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 92.2/210.1 kB 1.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 210.1/210.1 kB 1.6 MB/s eta 0:00:00\n",
            "Using cached langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "Using cached aiohttp-3.13.3-cp312-cp312-win_amd64.whl (455 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
            "Using cached langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "Using cached numpy-2.4.1-cp312-cp312-win_amd64.whl (12.3 MB)\n",
            "Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
            "Downloading sqlalchemy-2.0.46-cp312-cp312-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/2.1 MB 10.2 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.4/2.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.7/2.1 MB 5.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.1/2.1 MB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.0/2.1 MB 7.0 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.0/2.1 MB 6.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.1/2.1 MB 5.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 5.2 MB/s eta 0:00:00\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
            "Downloading greenlet-3.3.1-cp312-cp312-win_amd64.whl (227 kB)\n",
            "   ---------------------------------------- 0.0/227.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 227.2/227.2 kB 7.0 MB/s eta 0:00:00\n",
            "Using cached marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
            "Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, greenlet, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-openai, langchain-classic, langchain-community, langchain-experimental\n",
            "Successfully installed SQLAlchemy-2.0.46 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 attrs-25.4.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.3.1 httpx-sse-0.4.3 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-experimental-0.4.1 langchain-openai-1.1.7 marshmallow-3.26.2 multidict-6.7.0 mypy-extensions-1.1.0 numpy-2.4.1 propcache-0.4.1 pydantic-settings-2.12.0 typing-inspect-0.9.0 yarl-1.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install langchain-experimental langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "opSXvI2vGhE2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'RAG_TECHNIQUES'...\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S9k3LjGaGhE3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from RAG_TECHNIQUES.helper_functions import *\n",
        "from RAG_TECHNIQUES.evaluation.evalute_rag import *\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWLp4LX5GhE3"
      },
      "source": [
        "### Define file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_NRe6-VGhE3"
      },
      "outputs": [],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  201k  100  201k    0     0   353k      0 --:--:-- --:--:-- --:--:--  354k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  201k  100  201k    0     0   428k      0 --:--:-- --:--:-- --:--:--  429k\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!curl -o data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1lpWgQEyGhE4"
      },
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsMgN9c5GhE5"
      },
      "source": [
        "### Read PDF to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S8h7NivrGhE6"
      },
      "outputs": [],
      "source": [
        "content = read_pdf_to_string(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwt5FC-6GhE7"
      },
      "source": [
        "### Breakpoint types:\n",
        "* 'percentile': all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
        "* 'standard_deviation': any difference greater than X standard deviations is split.\n",
        "* 'interquartile': the interquartile distance is used to split chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XCR8k-P7GhE7"
      },
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rYvx4fiGhE8"
      },
      "source": [
        "### Split original text to semantic chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "crlX6Do9GhE8"
      },
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([content])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BleS_V8zGhE8"
      },
      "source": [
        "### Create vector store and retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4ql4TBXTGhE9"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF7GpCC_GhE9"
      },
      "source": [
        "### Test the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9HRTbFfVGhE9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context 1:\n",
            "The Intergovernmental Panel on Climate Change (IPCC) has \n",
            "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
            "provide a historical record that scientists use to understand past climate conditions and \n",
            "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
            "driven by human activities, particularly the emission of greenhouse gases. Chapter 2: Causes of Climate Change \n",
            "Greenhouse Gases \n",
            "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
            "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
            "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
            "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
            "activities have intensified this natural process, leading to a warmer climate. Fossil Fuels \n",
            "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
            "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
            "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
            "today. Coal \n",
            "Coal is the most carbon-intensive fossil fuel, and its use for electricity generation is a major \n",
            "source of CO2 emissions. Despite a decline in some regions, coal remains a significant \n",
            "energy source globally. It is mined extensively in countries like China, India, and the United \n",
            "States, contributing significantly to their energy supplies and CO2 footprints. Oil \n",
            "Oil is used primarily for transportation fuels, such as gasoline and diesel. The combustion of \n",
            "oil products releases significant amounts of CO2 and other pollutants, contributing to climate \n",
            "change and air quality issues. The global oil industry is vast, involving extraction, refining, \n",
            "and distribution, with significant geopolitical and economic implications. Natural Gas \n",
            "Natural gas is the least carbon-intensive fossil fuel and is often seen as a \"bridge fuel\" to a \n",
            "lower-carbon future. However, its extraction and use still contribute to greenhouse gas \n",
            "emissions, particularly methane, which is a potent greenhouse gas.\n",
            "\n",
            "\n",
            "Context 2:\n",
            "Understanding Climate Change \n",
            "Chapter 1: Introduction to Climate Change \n",
            "Climate change refers to significant, long-term changes in the global climate. The term \n",
            "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
            "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
            "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
            "contributed to climate change. Historical Context \n",
            "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
            "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
            "11,700 years ago marking the beginning of the modern climate era and human civilization. Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
            "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
            "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
            "unprecedented changes. Modern Observations \n",
            "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
            "and extreme weather events.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_query = \"What is the main cause of climate change?\"\n",
        "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
        "show_context(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8VhbYWPGhE9"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--semantic-chunking)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
